{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pa\n",
    "import numpy as np\n",
    "import pdb\n",
    "import sys\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    Helper Functions (do not edit!) \n",
    "\n",
    "The Tree_node class represents one NODE in a DecisionTree.\n",
    "\n",
    "Each NODE holds a left and a right child if it is not a leaf. \n",
    "\n",
    "If it is a leaf, it will contain the partition of the original dataset corresponding to the respective leaf.\n",
    "In a fully grown tree every leaf is pure with respect to the goal variable (=label). The label in this example is the \"rating\".\n",
    "\n",
    "Each NODE needs to have a split criterion that describes how the dataset is partitioned. It is a Python tuple containing:\n",
    "- The variable (\"feature\") in which the dataset is split (e.g. \"No. of doors\")\n",
    "- The cutoff value for the split (e.g. \"4\")\n",
    "- The goal variable for which the split is optimized (in this example: \"rating\"); it is according to this variable that the  metric (e.g. the gini gain) is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Tree_node:\n",
    "    original_Data = None\n",
    "    \n",
    "    def __init__(self,split=None,right_child=None, left_child=None):\n",
    "        self.split=split\n",
    "        self.right_child = right_child\n",
    "        self.left_child = left_child\n",
    "\n",
    "    # returns the child in which obs (=observation) belongs\n",
    "    def return_child(self,obs):\n",
    "        column = self.split[0]\n",
    "                        \n",
    "        if self.is_categorical(column):\n",
    "            if obs[column] == self.split[1]:\n",
    "                return self.right_child\n",
    "            else:\n",
    "                return self.left_child\n",
    "            \n",
    "        else:\n",
    "            if obs[column] >= self.split[1]:\n",
    "                return self.right_child\n",
    "            else:\n",
    "                return self.left_child\n",
    "    \n",
    "    # returns an estimate for the goal variable of obs\n",
    "    def classify(self, obs):\n",
    "        \n",
    "        child = self.return_child(obs)\n",
    "        \n",
    "        if child.__class__.__name__ == 'Tree_node':\n",
    "            return child.classify(obs)\n",
    "    \n",
    "        target_col = self.split[2]\n",
    "        if self.is_categorical(target_col):\n",
    "            #print(\"majority vote\")\n",
    "            return child[target_col].value_counts().keys()[0]\n",
    "        else:\n",
    "            #print(\"average\")\n",
    "            return np.average(child[~child[target_col].isnull()][\"age\"])\n",
    " \n",
    "    # determines if the column (\"feature\") is categorical (e.g., \"no. of doors\") or numerical (e.g., \"price\")\n",
    "    def is_categorical(self, column):\n",
    "        category=True\n",
    "        if not Tree_node.original_Data[column].dtype.name == \"category\":\n",
    "            category = False\n",
    "        return category\n",
    "    \n",
    "# returns the gini impurity for data with respect to column (i.e., use your goal variable as column here)    \n",
    "def gini_impurity(data, column, weights=None):\n",
    "    try:\n",
    "        counts = uniquecounts(data, column)\n",
    "        probs = counts/data.shape[0]\n",
    "        if len(probs) == 1:\n",
    "            prob_obs = np.ones(data.shape[0])\n",
    "        else:\n",
    "            la1 = lambda x: probs[probs.index == x][0]\n",
    "            prob_obs = np.array(list(map(la1, data[column])))\n",
    "            prob_obs = np.square(prob_obs)\n",
    "\n",
    "        if weights is None:\n",
    "            weights = np.ones(data.shape[0])\n",
    "        weights = weights/sum(weights)\n",
    "        return 1-sum(weights*prob_obs)\n",
    "    except:\n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "        raise\n",
    "       \n",
    "\n",
    "# Count ocurrences of every possible value for the given column\n",
    "def uniquecounts(data, column):\n",
    "   val_cnt = data[column].value_counts()\n",
    "   return val_cnt.drop(val_cnt[val_cnt == 0].index)\n",
    "\n",
    "\n",
    "# This is a helper function that partitions a dataset with respect to a given variable and a cutoff value\n",
    "def divideset(in_set, column, value):\n",
    "   # Make a function that tells us if a row is in the first group (true) or the second group (false)\n",
    "   split_function=None\n",
    "   if not in_set[column].dtype.name == \"category\":\n",
    "      # assume it to be numerical if not categorical\n",
    "      split_function=lambda in_set:in_set[column]>=value\n",
    "   else:\n",
    "      split_function=lambda in_set:in_set[column]==value\n",
    "                                   \n",
    "   # Divide the rows into two sets and return them\n",
    "   set1= in_set[split_function(in_set)].copy()\n",
    "   set2= in_set[np.invert(split_function(in_set))].copy()\n",
    "   return (set1,set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    Load Data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dat_car = pa.read_csv('car.data.csv', sep=\",\")\n",
    "dat_car.dtypes\n",
    "for i in dat_car.columns.values:\n",
    "    dat_car[i] = dat_car[i].astype('category')\n",
    "\n",
    "dat = dat_car\n",
    "target_col = \"rating\"\n",
    "\n",
    "# shuffle data\n",
    "np.random.seed(42)\n",
    "#dat = dat.reindex(np.random.permutation(dat.index))\n",
    "dat = dat.sample(frac=1)\n",
    "\n",
    "# split data into training and test set -> this is absolutely central to fitting a ML model!\n",
    "# If you are not sure why, ask your lecturer (it is likely to be on the final exam!)\n",
    "split = int(dat.shape[0]/100*20)\n",
    "                         \n",
    "dat_test = dat.iloc[0:split]\n",
    "dat_train = dat.iloc[(split+1):dat.shape[0]]\n",
    "\n",
    "Tree_node.original_Data = dat_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0\n",
    "Look at the dataset. \n",
    "The dataset contains information about used cars that are for sale, including a rating of the offer (last column in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1295, 7)\n",
      "['buy_price', 'maintenance', 'doors', 'persons', 'lug_space', 'safety', 'rating']\n",
      "['unacc', 'acc', 'vgood', 'good']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Please comment me",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f566a43d2b7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# In this exercise, we will fit a model to the remainig six variables in order to predict \"rating\", our goal variable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# You will have to add your own code at each failing assert like this one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Please comment me\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Please comment me"
     ]
    }
   ],
   "source": [
    "print(dat_train.shape)\n",
    "\n",
    "print(list(dat_train.columns))\n",
    "\n",
    "print(list(dat_train.rating.unique()))\n",
    "# In this exercise, we will fit a model to the remainig six variables in order to predict \"rating\", our goal variable.\n",
    "# You will have to add your own code at each failing assert like this one\n",
    "assert False, \"Please comment me\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Fit a tree stump: in this task you will fit a tree stump (i.e., a Decision Tree of depth 1, only containing a single split) to the data. You will have to find the partition of the input dataset that yields the biggest gini score gain with respect to the goal variable. You will accomplish this by exhaustive search, i.e., by trying every possible partition.\n",
    "\n",
    "# Input Parameters:\n",
    "1. in_set: the training data (features)\n",
    "2. traget_col: the ground truth / labels (here: the \"rating\" column)\n",
    "3. weights: weight of each sample (can be ignored at the beginning) \n",
    "\n",
    "# Outputs:\n",
    "1. best_split: a tuple including the name of the most discriminant (best) feature for splitting, the corresponding threshold, and the name of the label column (see also comment above at \"Helper functions\"\n",
    "2. best_sets: a tuple of two sets, which are the outputs of the \"divideset\" function; these sets are the result of dividing the training data based on the best split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_split(in_set, target_col, weights=None):\n",
    "    # compute the gini score for the unpartitioned dataset\n",
    "    in_score = gini_impurity(in_set, target_col,weights)\n",
    "\n",
    "    best_gain = 0\n",
    "    best_split = None\n",
    "    best_sets = None\n",
    "    \n",
    "    # try every column\n",
    "    for act_col in in_set.columns.values:\n",
    "        # ignore goal variable - otherwise its trivial\n",
    "        if act_col == target_col: continue\n",
    "        print(act_col)\n",
    "        # construct a list of unique values for this variable \n",
    "        column_values = set(in_set[act_col])\n",
    "\n",
    "        assert False, \"find_best_split not implemented yet\"\n",
    "        # Try every possible split of the dataset w.r.t. the current variable\n",
    "        # save the split that yielded the hightest gini gain\n",
    "        # The gini-gain of a partition is defined as follows (assume the original set is partitioned into part_1 and part_2):\n",
    "        #   gain = gini_impurity(set) - p_1*gini_impurity(part_1) - (1-p_1)*gini_impurity(part_2)\n",
    "        # where p_1 = nrows(part_1)/nrows(set)\n",
    "        # Hint: see Tree_node comments for the definition of the split\n",
    "\n",
    "  \n",
    "    return best_split, best_sets\n",
    "\n",
    "\n",
    "# Fitting a stump is trivial when you have found the best split\n",
    "split, sets = find_best_split(dat_train, target_col)\n",
    "stump = Tree_node(split, sets[0], sets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Compute the confusion matrix and the correct classification percentage for your tree.\n",
    "\n",
    "# Input Parameters:\n",
    "1. in_data: data samples (features)   \n",
    "2. target_col: nameof column containing data labels \n",
    "3. tree: a trained tree (or stump) for evaluating the samples\n",
    "\n",
    "# Outputs:\n",
    "1. conf_mat: confusion matrix of the decisions based on the input data and given tree  \n",
    "2. p_correct: probability of correct decisions (also called accuracy of the classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conf_matrix(in_data, target_col, tree):\n",
    "  \n",
    "    # distinct values available in the target variable (in this example: 4 levels of accpetability of the car)\n",
    "    levels = uniquecounts(in_data, target_col).keys()\n",
    "    # confusion matrix itself\n",
    "    conf_mat =  np.zeros((len(levels),len(levels)))\n",
    "    # percentage of correct classifications\n",
    "    p_correct = 0\n",
    "    \n",
    "    \n",
    "    assert False, \"compute confusion matrix and p_correct here\"\n",
    "    for index, row in in_data.iterrows():\n",
    "\n",
    "    \n",
    "    \n",
    "    return conf_mat, p_correct\n",
    "\n",
    "# Build confusion matrix with training data\n",
    "conf_mat_stump_train, p_correct_stump_train = conf_matrix(dat_train, target_col,stump)\n",
    "\n",
    "# Build confusion matrix with test data\n",
    "conf_mat_stump_test, p_correct_stump_test = conf_matrix(dat_test, target_col,stump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Recursively build a tree of variable depth.\n",
    "\n",
    "# Input Parameters:\n",
    "1. in_data: training data (features)\n",
    "2. traget_col: name of column within in_data that contains the ground truth labels\n",
    "3. max_depth: maximum depth of the tree\n",
    "3. weights: weight of the samples\n",
    "\n",
    "# Output:\n",
    "An instance of the \"Tree_node\" class, initialized by a split (output of the \"find_best_split\" function) in addition to a recursive call of the \"train_tree\" function for two subsets of training data in the right and left children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_tree(in_data,target_col,max_depth=99, weigths = None):\n",
    "    # To recursively build a decision tree, you have to do two things:\n",
    "    # - if you hit your stopping criterion, just return in_data (there are two optional criteria to stop the recursion)\n",
    "    # - otherwise, find the best split and call this method on both partition sets\n",
    "    assert False, \"recursively build tree here\"\n",
    "\n",
    "    return Tree_node(split,right_child,left_child)\n",
    "\n",
    "# Build a tree of depth 5 (a fully grown tree is pretty slow, but you can play around with the depth parameter and have a look\n",
    "# at its influence on the classification performance)\n",
    "depth5_tree = train_tree(dat_train, target_col, 5)\n",
    "\n",
    "# Build confusion Matrix with training data\n",
    "conf_mat_5_train, p_correct_5_train = conf_matrix(dat_train, target_col,depth5_tree)\n",
    "\n",
    "# Build confusion Matrix with test data \n",
    "conf_mat_5_test, p_correct_5_test = conf_matrix(dat_test, target_col,depth5_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "In this task, you repeat task 3 and 2, this time using the popular ML library scikit-learn instead of your own custom implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding of the target variable (label): the data has to be encoded as described below for sklearn. Don't worry about the\n",
    "# next 3 lines - just use the _encoded version when you pass data to sklearn\n",
    "d = defaultdict(LabelEncoder)\n",
    "dat_train_encoded = dat_train.apply(lambda x: d[x.name].fit_transform(x))\n",
    "dat_test_encoded = dat_test.apply(lambda x: d[x.name].transform(x))\n",
    "\n",
    "# Hints: Have a look at the DecisionTreeClassifier class. Use criterion=\"gini\" and max_depth=5 to make the results \n",
    "# comparable to task 3. You might have to take a look at the sklearn documentation.\n",
    "# Attention: If you pass data to sklearn, you have to remove the target variable - otherwise sklearn will use it for \n",
    "# the prediction (e.g. use: dat_train_encoded[dat_train_encoded.columns.difference([target_col])] as training data)\n",
    "assert False, \"Allocate a decision tree, fit it to the training data and compute the predictions for the goal variable using sklearn\"\n",
    "\n",
    "\n",
    "\n",
    "# Inverse the encoding on the predictions and compute the confusion rate --> how does this compare to your own implementation?\n",
    "predictions = d[target_col].inverse_transform(predictions)\n",
    "sum(dat_test[target_col] == predictions)/float(len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "In this task, you will implement the AdaBoost algorithm to fit a number of trees and use their collective power\n",
    "to build a better classifier. While you can use your own tree implementation, we advise you to use DecisionTreeClassifier\n",
    "for performance reasons (one run of AdaBoost will fit 50 trees by default). Using the following function, you will fit decision trees to the data using Adaboost:\n",
    "\n",
    "# Input Parameters (ada_boost_trees):\n",
    "1. in_data: training data (features)\n",
    "2. column: name of column containing the ground truth labels \n",
    "3. depth: depth of the individual trees\n",
    "4. m: number of trees (hypotheses) to fit\n",
    "\n",
    "# Outputs (ada_boost_trees):\n",
    "1. trees: a list of fitted trees\n",
    "2. importance: a list of the respective importances (weights) for the fitted trees; these values are used for final weighted decisions \n",
    "\n",
    "# Input Parameters (predict_boosted_trees):\n",
    "1. trees: a list of the fitted trees\n",
    "2. importance: a list of the respective importances (weights) for the fitted trees; these values are used for final weighted decisions \n",
    "3. obs: data samples (\"observations\") used for evaluation\n",
    "\n",
    "# Output (predict_boosted_trees):\n",
    "1. trees: a list of the fitted trees\n",
    "2. importance: a list of the respective importances (weights) for the fitted trees; these values are used for final weighted decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ada_boost_trees(in_data, column, depth, m):\n",
    "    trees = []\n",
    "    importance = []\n",
    "    \n",
    "    N, _ = in_data.shape\n",
    "    # initialize weights uniformly\n",
    "    w = np.ones(in_data.shape[0]) * float(1)/in_data.shape[0]\n",
    "    \n",
    "    for k in range(m):\n",
    "        # fit tree using actual weights\n",
    "        d_tree = DecisionTreeClassifier(criterion = \"gini\", max_depth=depth)\n",
    "        d_tree = d_tree.fit(in_data[in_data.columns.difference([target_col])],  in_data[target_col], sample_weight=w)\n",
    "        predictions = d_tree.predict(in_data[in_data.columns.difference([target_col])])            \n",
    "\n",
    "        # compute the weighted error\n",
    "        # i.e. sum up w but leave out each value for which the prediciton is correct\n",
    "        assert False, \"compute weighted_err here\"\n",
    "\n",
    "        \n",
    "        # stop if there are no errors\n",
    "        if weighted_err == 0:\n",
    "            break\n",
    "            \n",
    "        # model importance\n",
    "        model_imp = np.log((1-weighted_err)/(weighted_err))\n",
    "        \n",
    "        # update weights\n",
    "        # Option1 \"by a linear factor\": for each correctly labeled sample: w = w * weighted_err/(1-weighted_err) \n",
    "        # Option2 \"by exp(influence) in case of error\": for each  misclassified sample: w = w*exp(model_imp) \n",
    "        # Option3: Can you think of a better way to \"boost the weights\"\n",
    "        # normalize w\n",
    "        assert False, \"update w here\"\n",
    "\n",
    "        \n",
    "        trees.append(d_tree)\n",
    "        importance.append(model_imp)\n",
    "        \n",
    "    return trees, importance\n",
    "\n",
    "# predicts the class of a number of observations, based on trees and importances returned by the above method\n",
    "def predict_boosted_trees(trees, importance, obs):\n",
    "    N, _ = obs.shape\n",
    "    \n",
    "    predictions_dir = dict()\n",
    "    \n",
    "    for (tree, model_imp) in zip(trees, importance):\n",
    "        if model_imp == 0: continue\n",
    "\n",
    "        predictions = tree.predict(obs)\n",
    "        levels = set(predictions)\n",
    "\n",
    "        for level in levels:\n",
    "            if level in predictions_dir.keys():\n",
    "                predictions_dir[level] += (predictions == level)*(model_imp)\n",
    "            else:\n",
    "                predictions_dir[level] = (predictions == level)*(model_imp)\n",
    "    \n",
    "    pred = np.zeros((N,len(predictions_dir.keys())))\n",
    "    \n",
    "    for k in predictions_dir.keys():\n",
    "        pred[:,k]=predictions_dir[k]\n",
    "    \n",
    "    return np.argmax(pred, axis=1)\n",
    "\n",
    "# based on the code in predict_boosted_trees, how does the prediction based on a number of boosted trees work?\n",
    "assert False, \"write in own words - 2 to 3 sentences\"\n",
    "\n",
    "# train boosted trees\n",
    "trees, importance = ada_boost_trees(dat_train_encoded, target_col, 5, 50)\n",
    "\n",
    "# predict using boosted trees\n",
    "predictions = predict_boosted_trees(trees, importance,dat_test_encoded[dat_test_encoded.columns.difference([target_col])])\n",
    "predictions = d[target_col].inverse_transform(predictions)\n",
    "\n",
    "# compute prediction accuracy of the boosted trees\n",
    "sum(dat_test[target_col] == predictions)/float(len(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "Compare our Adaboosted Trees versus sklearn. Fit an sklearn AdaBoostClassifier using a DecisionTreeClassifier as base classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "assert False, \"define, fit an AdaBoostClassifier and use it do make predictions\"\n",
    "\n",
    "\n",
    "predictions = d[target_col].inverse_transform(predictions)\n",
    "sum(dat_test[target_col] == predictions)/float(len(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
